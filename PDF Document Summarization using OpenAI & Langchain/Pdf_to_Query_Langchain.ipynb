{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Blessy Chinthapalli"
      ],
      "metadata": {
        "id": "fsS9n8RqdjEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Project is focused on creating a system to query content from PDF documents using Langchain and OpenAI's embeddings"
      ],
      "metadata": {
        "id": "CEiNv23Mdooh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Libraries and Dependencies:\n",
        "\n",
        "\n",
        "*   Langchain: A library that helps with vectorization of text,\n",
        "allowing you to work with text embeddings, which are numerical representations of text used for comparing and searching through content.\n",
        "*   OpenAI: Used to obtain text embeddings that capture the semantic meaning of text, enabling tasks like similarity search.\n",
        "\n",
        "\n",
        "\n",
        "*   PyPDF2: A Python library for reading and extracting text from PDF files.\n",
        "*   FAISS: A library for efficient similarity search, particularly useful when working with large collections of embeddings.\n",
        "\n",
        "\n",
        "*   Tenacity: A library that helps with retry mechanisms, useful for making API calls more robust by automatically retrying failed requests.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mcgaIHihd_8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF Query Using Langchain"
      ],
      "metadata": {
        "id": "gBtk_tC8zmC1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rdAYZepFhJM",
        "outputId": "92d328c7-42dc-4aa2-f6c0-296eb38d99fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.12)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.29)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain) (3.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.40.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.5.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain # to vectorize\n",
        "!pip install openai # we use oopen ai embeddings - measures the relatedness of text\n",
        "!pip install PyPDF2 # to read from pdf\n",
        "!pip install faiss-cpu\n",
        "!pip install tiktoken # dependency library for pdf to create tokens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZf7pqNtUmSB",
        "outputId": "dc5f5fa2-2457-410b-ec08-8961796e2e27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.10.1)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.12)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.29)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.99)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-community-0.2.11 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter # to split the content\n",
        "\n",
        "from langchain.vectorstores import FAISS # vector database to store"
      ],
      "metadata": {
        "id": "v8fCmC-6Q3pP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xP1-3VjZdlf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
        "# os.environ[\"SERPAPI_API_KEY\"] = \"\" # to do google search, but we are not doing a google search or implimentingc a chatbot"
      ],
      "metadata": {
        "id": "_aQ7ps_dRJOq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# provide the path of  pdf file/files.\n",
        "pdfreader = PdfReader('BlessyCH_EY_Resume.pdf')"
      ],
      "metadata": {
        "id": "_FA1ZERdRLAM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading PDF Content:\n",
        "The PdfReader from PyPDF2 is used to read a PDF file. It iterates through the pages of the PDF, extracting text from each page and combining it into a single raw text string."
      ],
      "metadata": {
        "id": "hxR00Qb4eaWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content"
      ],
      "metadata": {
        "id": "q9AeO9cDRqMj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "yGlxUMl-Rsmy",
        "outputId": "e3df1721-46f9-4a63-b60a-5b6e705b7a27"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"BlessyChinthapalliSanFrancisco,CA+1(415)225-1956blessychinthapalli20@gmail.comLinkedIn|Github|Tableau\\nHiringManagerErnst&YoungLLP\\nDearHiringManager,\\nIamwritingtoexpressmyenthusiasmfortheDataEngineer,DataAnalystpositionsatErnst&YoungLLP.Witharobustbackgroundindataengineering,machinelearning,andbusinessintelligence,coupledwithhands-onexperienceacrossdiverseprojects,IameagertocontributetoEY’svisionofbuildingabetterworkingworld.\\nMyjourneyindatascienceandengineeringbeganduringmyBachelorofEngineeringinElectronics&CommunicationfromShivNadarUniversity,whereIdevelopedakeeninterestindata-drivensolutions.MyacademicpursuitsculminatedinaMasterofScienceinBusinessAnalyticsfromtheUniversityofCalifornia,Davis,whereIhonedmyskillsindatamodeling,visualization,andmachinelearning.\\nAtExPrep,asaDataEngineerBusinessConsultant,Iwaspivotalinrevolutionizingtheirdatavisualizationcapabilities.Theypossessedbillionsofstudentassessmentdatabutlackedaneffectivewaytoharnessthiswealthofinformation.Collaboratingcloselywiththefounder,IdevelopedcomprehensivePowerBIandTableaudashboardstomonitorstudentmetrics,significantlyenhancingengagementandallowingprofessorstoredesignassignments,reducinganalyticstimeby50%.Thisinitiativenotonlyaddressedimmediatebusinessneedsbutalsoenrichedtheuserexperience,leadingtoa20%increaseinclientturnoverratebyintegratingthesedashboardsintotheirexistingportal.Designedandimplementedsolutionsfordataaggregation,improveddatafoundationalprocedures,integratednewdatamanagementtechnologiesandsoftwareintotheexistingsystemandbuiltdatacollectionpipelines.\\nAtHewlettPackard(HPInc),ItookonthechallengeoftransformingandorchestratingcustomerservicedataacrosstheentirePAN-HPdatabase.Thisrolewaspivotal,asitinvolvednotonlythemigrationofdatafromon-premisessystemstotheAzurecloudbutalsoensuringdataqualityandgovernance.TheprojectaimedtobuildaFederatedDataLake(FDL)usingaDataMeshapproach,enablingbusinessunitstoaccessdomain-orienteddataefficientlyintheconsumptionlayerthroughDatabricks,PySpark,andUnityCatalog.\\nPriortothistransformation,ourdatainfrastructurewasbasedonanEnterpriseDataLake(EDL)hostedonon-premisesHadoop(Hive)andTalend.ThegoalwastotransitiontoaCloudDeltaLaketoreducecosts,enhancesupportability,andprovidefasterdataavailabilityfromsourcetobusinessusers.ThismigrationrequiredcreatinglogicaldatamodelsonAzureCloudandmaintainingrigorousdataqualitystandards.Theresultwasamoreagile,cost-effective,androbustdataecosystemthatsignificantlyimprovedourabilitytoderiveinsightsandsupportbusinessdecisions.Inmyrole,Iwasresponsibleforend-to-enddatamanagement,encompassingtheentiredatalifecyclefromacquisitionandprocessingtodelivery.Thisinvolvedtranslatingcomplexbusinessrequirementsintotechnicalspecifications,buildingandmaintainingETL/ELTpipelines,andensuringdataavailabilityforreportingandanalyticsteams.Wheneverdiscrepanciesarose,Idiligentlytracedtherootcauses,resolvedissues,andredeployedthedatapipelinestomaintainseamlessoperations.\\nCollaborationwasakeyaspectofmyroleatHP.Iworkedcloselywithcross-functionalteamsinbothITandbusinessunits,managedscrumcalls,andfosteredacollaborativeenvironmentthatenhancedourdevelopmentprocesses.\\nAtVeraLifecare,asaDataAnalyst,Iconductedexploratorydataanalysis,developeddashboards,andreducedstockoutoccurrences,directlyimpactingoperationalefficiencyandcustomersatisfaction.\\nOneofmynotableprojectsinvolvedworkingwithNike,whereIdevelopedacomprehensivemachinelearningsolution.Iweb-scrapeddatafromNike’sonlinewebsite,designedarobustdatamodel,performeddatawranglinganddataingestionintoMongoDB,andutilizedthisdataasthefoundationforbuildingpredictivemodelsandtextclassificationusingNLPtechniques.Thisprojectculminatedindevelopingarecommendationsystemthatprovidedpersonalizedproductsuggestionstousers,showcasingmyabilitytomanagethefulldatapipelineandapplyadvancedanalyticstodeliverimpactfulbusinesssolutions.\\nMytechnicalproficienciesspanacrossSQL,Python,R,Scala,andMatlab,withextensiveexperienceincloudplatformssuchasAzure,GCP,andDatabricks.IamalsoskilledinBItoolslikeTableau,PowerBI,andLooker.Mycertifications,includingDatabricksCertifiedDataEngineerAssociateandMicrosoftAzureFundamentals,attesttomycommitmenttocontinuouslearningandprofessionalgrowth.\\nEY’sreputationforexcellenceanditscommitmenttofosteringinnovationalignperfectlywithmycareeraspirations.IamexcitedabouttheopportunitytobringmyexpertiseindatascienceandanalyticsandmypassionfortechnologytoEY,contributingtoimpactfulsolutionsforclientsinthefinancialservicessector.\\nThankyouforconsideringmyapplication.Ilookforwardtothepossibilityofdiscussinghowmybackground,skills,andcertificationscancontributetotheinnovativeprojectsatEY.Pleasefindmyresumeattachedforyourreview.\\nWarmregards,\\nBlessyChinthapalliBlessyChinthapalli\\nDataEngineerII/SeniorDataAnalyst\\n(415)225-1956\\n◆blessychinthapalli20@gmail.com\\n◆SanFrancisco,CA\\n◆LinkedIn\\n◆Github\\n◆Tableau\\nPROFESSIONALSUMMARY\\nVersatileDataprofessionalwithover5yearsofexperienceindesigningandoptimizingsolutionsforlarge-scaledata.Stronganalyticalskills,capableofderivinginsightsfromcomplexdatasetstosupportbusinessdecisionmaking.Provenleadershipinmanagingteamsanddrivingprojectstosuccessfulcompletionwithaproactiveapproach.\\nSKILLSMachineLearning,ExperimentalDesign,DataModelling&Visualisation,ETL/ELT,AdHocDataAnalysis,StatisticsProgramming:SQL,Python,R,Scala,Scala,BigQueryDatabases:MySQL,Oracle,MongoDB,CassandraCloudPlatforms:Azure,GCP,Databricks,AWSETL/ELT:Pipelines,DataLake,Warehouse,CI/CDBigData:Spark,Kafka,Hadoop,Airflow,DBTBITools:Tableau,PowerBI,Looker,Snowflake\\nCERTIFICATIONSDatabricksCertifiedDataEngineerAssociateMicrosoftAzureFundamentals(AZ900)TableauDesktopSpecialistGoogleDataAnalyticsProfessionalAssociateCertifiedAnalyticsProfessional(INFORMS-aCAP)\\nEDUCATION\\nUniversityofCalifornia,DavisSanFrancisco,CAMasterofScience,BusinessAnalytics06/2024ShivNadarUniversityDelhiNCR,INDBachelorofEngineering,Electronics&CommunicationEngineering06/2021\\nPROFESSIONALEXPERIENCE\\nExPrepCharlotte,NC,RemoteDataAnalystBusinessConsultant09/2023–06/2024\\n●DevelopedPowerBiandTableaudashboardstomonitorstudentmetricssignificantlyimprovingengagementandenablingprofessorstoredesignassignments,reducinganalyticstimeby50%\\n●Translatedcomplexbusinessrequirementsintodatamodels,enrichingtheuserexperiencebyintegratingdashboardfeaturesintotheirportalresultingina20%increaseinclientturnoverrate.\\n●Implementedarealtimedatapipelinetoprocessunstructureddatabyintegrating30Mrawrecords\\nHewlettPackard(HPInc)Bengaluru,INDDataEngineerII,AnalyticsandDataScienceSolutions10/2022–07/2023C o l l a b o r a t e dw i t hD a t aO r g .a n dB It e a m st ot r a n s f o r mP A N-H Pc u s t o m e rs e r v i c ed a t a&o nM i g r a t i o np r o j e c t s\\n●Demonstratedexpertiseindefiningbusinessteamsrequirements,leveragingdataandmetricstoderiveinsights,andmakingimpactfulrecommendationsforupstreamanddownstreamproduct/consumerteams\\n●Optimizeddataacquisitionpipelinesfordiversedataformats(streaming,transactional,semi-structured)intoDatalake(ADLSgen2)andcloudtechnologiesfrom15+sources,resultingina99.8%uptime&a45%reductioninjobexecutiontimes\\n●Ledtheend-to-endmigrationoflegacysystems(Hadoop,Hive)toAzure,ensuringseamlessdatatransition,integration,andenhancementofdataprocessingcapabilitiescollaboratingwith10+cross-functionalteams●DevelopedPyspark,SQLscripts&batchprocessingjobsfortransformationof10Tbdistributeddatadaily\\n●EmployedUnityCataloginDatabrickstoenhancedatagovernanceandcomplianceensuringconsistentdatamanagementacrosswhileleveragingApacheSparkforhigh-performancedatatransformations\\n●ManagedfullSDLC,continuousdeployments,UAT,hypercareusingAgileScrumMethodology,DevOps,Jira\\n●TrainedandmentoredteammembersandvendorsgivingKT’s,fosteringacultureofcontinuouslearning\\nDataEngineerI,AnalyticsandDataScienceSolutions09/2021–10/2022S u b j e c tm a t t e re x p e r tt od e f i n eg l o b a lc u s t o m e rt e l e m e t r yd a t aq u a l i t ye x p e c t a t i o n sf o rd o w n s t r e a m s\\n●Achieved$1.2Mincostsavingsthroughoptimizationofingestionpipelinesandrepositoryframeworks\\n●Demonstratedin-depthtechnicalcapabilitiesandprofessionalknowledge,maintaininglong-termclientrelationshipsandnetworks,andcultivatingbusinessdevelopmentopportunities\\n●DevelopedETLscripts,storedprocedures,integrationtesting&technicaldataqualitychecksinSynapse.\\n●Establisheddata-drivenproductdevelopmentculturebydrivingthedefinition,tracking,andoperationalizingofproductmetricsandincreasingdataaccessibility.\\n●Skilledindebuggingdataissues,identifyingrootcauses,andimplementingeffectivesolutions\\nBusinessIntelligenceIntern,Analytics&BIOperations02/2021–08/2021\\n●BuiltautomatedReconciliationPowerBIDashboardsfortrackingdataflowacrossmedallionarchitecture\\n●ProficientlycrafteddatamodelsforRDBMSandNoSQLstores,enhancedqueryretrievalby30%\\n●Analyzedlargedatasetstoidentifytrends,patterns,andanomalies\\n●Designedproductroadmaps,providedself-servingdatasolutions,&assistedproductowners\\nVeraLifecareVisakhapatnam,INDDataAnalyst05/2019–08/2020\\n●ConductedExploratoryDataAnalysis,unveilingsalespatternsthatboostedinventoryturnoverby15%.\\n●Generatedsaleslossforecastsfromstockouts,reducingstockoutoccurrencesby20%.\\n●Developeddashboardsforoperationalandmanagementreporting,fordatadrivendecisions\\nPROJECTS\\n●NikePredictiveAnalyticsandPatternRecognition:WebScrappedanddesignedsmallscaleendtoenddatamodel,performeddatawranglingusingseleniumandfeatureengineeringfortextmodeling\\n●DataLakehouseImplementation:DevelopedETLpipelinestoprocessandcleanrawdatastoredinADLS,integratedwithSynapseforanalyticalprocessing,usedDeltaLakeforACIDtransactions&schemaenforcement.\\n●NLP:MachineLearningProjectsusingDataSciencetechniquessuchastimeseries,forecasting,classification,regression,clustering,modelevaluation,predictivemodeling.\\nAWARDS&ASSOCIATIONS\\n●UCDavisDean'sScholarship\\n●MSBAFellowshipAward\\n●TableauUserGroup(GuestSpeaker)\\nCloudServicesandPlatforms:,AzureDataFactory,Azureservices,jenkins,linux,vertica,docker\\nProgrammingLanguag esandTools:C/C+ +,Java,Node,HTML ,Rust,JSON,Pytorch,API,Javascript,C#,OLAP,OLTP,kanban,bash,REST,Powershell,kubernetes\\nDataToolsandTechnologies:Ex cel,Look er,Pandas,Nump y,ApacheSpark,Da taExplorer,PostgreSQL,Cassandr a,Oracle,Parquet\\nDevOpsandInfrastructure:Lambda,Me tadata,Versioncontrol,Automatedtesting,Sourcecontrolmanagement,Buildprocesses,Oper ations,Unix,K eyVault,TensorFlo w\\nDataManagement|datawarehouse:Da tastructures,Datavalidation,ETLpipelines,Da taandanalytics,database,teradata,datamining,datamodeling ,datavisualization\\nBusinessandProductDevelopmen t:Businessperformance,Pr oductManagement,Healthc are,Marketing,Finance,R OIanalysis,eCommer ce,Leader ship,sharepoint\\nMachineLearning:A/Btesting,Hypothesistesting,Costtrend,Growth-orien ted,Interpersonalskills,Artificialintelligence,computervision,LargeLanguag eModelsLLM,DeepLearning\\nProjectandClientManagement:ProjectManagement,Clientmanagement,Compliance(includingdocumen tation,codingstandards,codereviews,Communic ationskills\\nFrontendandBackendDevelopmen t:NLP,fullstack,advancedanalytics,bigdatatechnologies,dataarchitecture,dataingestion,dataflow,unity,processimprovement\\nAdditionalToolsandTechnologies:SaaS,S AS,Flink,F abric,ASA,apachekafka,microservices,CloudComputing ,userfeedback,adtech,fintech,kpi,AI,dataparsing,datareporting\\nMiscellaneous:Sc alablesystems,Pivottables,Enterprise,businessobjectiv es,webservices,statistics,rootcauseanalysis,timemanagement,opensource,productanalytics,mathematics\\ncapitalmarkets,prototypes,Pig,Hive,cloudtechnologies,jQuery,thoughtleadership,ERP,DynamoDB ,SAP,operationalimprovements\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Splitting:\n",
        "Since the extracted text can be lengthy, it's split into smaller chunks using CharacterTextSplitter.\n",
        "\n",
        "OpenAI's, have token limits, so the text needs to be split into manageable chunks.\n",
        "\n",
        "The chunk_size defines the maximum length of each chunk, while chunk_overlap allows some overlap between chunks to maintain context."
      ],
      "metadata": {
        "id": "iUsuOZXmepKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)"
      ],
      "metadata": {
        "id": "VP6ap_PSRt7s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ea21eb2-389f-4a01-d83b-06cd28f6c438"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 926, which is longer than the specified 800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9GLXwH1SVOe",
        "outputId": "d12cf612-05ae-400c-8f09-2a91fedb3ae6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Embeddings:\n",
        "OpenAI Embeddings: The code downloads embeddings from OpenAI, which are used to convert the text chunks into numerical vectors. These embeddings are essential for later querying or searching through the text based on semantic meaning."
      ],
      "metadata": {
        "id": "3wrLVP8ae0Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "wqy4vJhrSXUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4269f27-b2be-435c-e020-ab3e2c58e01d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tenacity\n",
        "import time\n",
        "from tenacity import (\n",
        "    retry,\n",
        "    stop_after_attempt,\n",
        "    wait_random_exponential,\n",
        ")  # for exponential backoff\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6QRqR_EcyHW",
        "outputId": "e7925e90-6037-445f-d132-aed68059bc04"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (8.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings:** Numerical representations of text that capture its semantic meaning. These are crucial for tasks like text similarity, clustering, and search."
      ],
      "metadata": {
        "id": "91wbDKn-e9ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings(allowed_special=['<|endofprompt|>'])\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def embed_with_retry(texts, embeddings):\n",
        "    # Add a delay here if needed\n",
        "    time.sleep(1)  # Example: wait for 1 second between calls\n",
        "    document_search = FAISS.from_texts(texts, embeddings)\n",
        "    return document_search\n",
        "\n",
        "document_search = embed_with_retry(texts, embeddings)"
      ],
      "metadata": {
        "id": "5jXo6rJkXr7c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Database (FAISS):\n",
        "\n",
        "A database that stores vectors (embeddings) and allows for efficient similarity search. In this case, FAISS is used to store and query the embeddings of the text chunks."
      ],
      "metadata": {
        "id": "66fJ2p3dfGKX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_search = FAISS.from_texts(texts, embeddings)"
      ],
      "metadata": {
        "id": "3igYiWjISjvS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYsMkdVlUd_V",
        "outputId": "da94f8a4-62a3-4953-e478-06ba66a60a3d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langchain_community.vectorstores.faiss.FAISS at 0x79b6887439a0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bpcafTEWk534"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"blessy previous employer\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "7sjc1Xh2SsTs",
        "outputId": "14a3ff6a-0275-4dfa-b5ea-44737b7db051"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' VeraLifecare'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what are blessy's skills\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "x9bWVQu4VQwq",
        "outputId": "8db5cd84-6f4a-4cfb-a78e-af1997bcd168"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Blessy's skills include data engineering, business intelligence, data modeling, machine learning, SQL, Python, R, Scala, Matlab, cloud platforms such as Azure and GCP, BI tools like Tableau, PowerBI, and Looker, and certifications in Databricks and Microsoft Azure.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Legal Document Summarization with Langchain\n",
        "## Overview:\n",
        "This script extracts, processes, and summarizes content from legal documents (e.g., lease contracts) using Langchain and OpenAI embeddings. It efficiently identifies and retrieves key information for faster document review."
      ],
      "metadata": {
        "id": "st2Hagh2fw6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### For reading big PDF's\n",
        "\n",
        "pdfreader = PdfReader('Lease_Agreement_178_Bluxome_.pdf')\n",
        "\n",
        "# read text from pdf\n",
        "raw_text = ''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "    content = page.extract_text()\n",
        "    if content:\n",
        "        raw_text += content\n",
        "\n",
        "\n",
        "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator = \"\\n\",\n",
        "    chunk_size = 800,\n",
        "    chunk_overlap  = 200,\n",
        "    length_function = len,\n",
        ")\n",
        "texts = text_splitter.split_text(raw_text)\n",
        "\n",
        "\n",
        "\n",
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Download embeddings from OpenAI\n",
        "embeddings = OpenAIEmbeddings(allowed_special=['<|endofprompt|>'])\n",
        "\n",
        "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
        "def embed_with_retry(texts, embeddings):\n",
        "    # Add a delay here if needed\n",
        "    time.sleep(1)  # Example: wait for 1 second between calls\n",
        "    document_search = FAISS.from_texts(texts, embeddings)\n",
        "    return document_search\n",
        "\n",
        "document_search = embed_with_retry(texts, embeddings)\n",
        "\n",
        "\n",
        "document_search = FAISS.from_texts(texts, embeddings)\n",
        "\n",
        "query = \"Summarise the lease agreement in 3 lines\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "oBWxNNNbYwS-",
        "outputId": "6ea1a94f-a60b-4a42-95c5-cc77da162103"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1084, which is longer than the specified 800\n",
            "WARNING:langchain_text_splitters.base:Created a chunk of size 1170, which is longer than the specified 800\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" The lease agreement requires the purchaser to submit an affidavit to the condominium owners' association before the sale can be finalized. It is also a condition of closing that the potential purchaser must execute the affidavit. The governing documents state that the project is subject to the Notice of Special Restrictions and any changes to windows may be required for future construction.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"when does the bluxome lease end\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LcZxiO-_ZdlV",
        "outputId": "2019ba17-8a61-404d-a05b-65daeaaf364e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The Bluxome lease ends on 1/16/2025.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"who are there on the lease give names\"\n",
        "docs = document_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C-eauJoDZ6Hz",
        "outputId": "427f142a-11d3-497c-c6ba-cdd019f58559"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Shriya Yegalapati, Blessy Chinthapalli, Aditya Satpute, and Daniel Lau are listed as tenants on the lease.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}