# -*- coding: utf-8 -*-
"""Pdf to Query Langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XuY3SvyE61Z7AodH0-BMcE97u982BRA7

Blessy Chinthapalli

This Project is focused on creating a system to query content from PDF documents using Langchain and OpenAI's embeddings

### 1. Libraries and Dependencies:


*   Langchain: A library that helps with vectorization of text,
allowing you to work with text embeddings, which are numerical representations of text used for comparing and searching through content.
*   OpenAI: Used to obtain text embeddings that capture the semantic meaning of text, enabling tasks like similarity search.



*   PyPDF2: A Python library for reading and extracting text from PDF files.
*   FAISS: A library for efficient similarity search, particularly useful when working with large collections of embeddings.


*   Tenacity: A library that helps with retry mechanisms, useful for making API calls more robust by automatically retrying failed requests.

## PDF Query Using Langchain
"""

!pip install langchain # to vectorize
!pip install openai # we use oopen ai embeddings - measures the relatedness of text
!pip install PyPDF2 # to read from pdf
!pip install faiss-cpu
!pip install tiktoken # dependency library for pdf to create tokens

pip install -U langchain-community

from PyPDF2 import PdfReader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter # to split the content

from langchain.vectorstores import FAISS # vector database to store

import os
os.environ["OPENAI_API_KEY"] = "OPENAI_API_KEY"
# os.environ["SERPAPI_API_KEY"] = "" # to do google search, but we are not doing a google search or implimentingc a chatbot

# provide the path of  pdf file/files.
pdfreader = PdfReader('BlessyCH_EY_Resume.pdf')

"""### Reading PDF Content:
The PdfReader from PyPDF2 is used to read a PDF file. It iterates through the pages of the PDF, extracting text from each page and combining it into a single raw text string.
"""

from typing_extensions import Concatenate
# read text from pdf
raw_text = ''
for i, page in enumerate(pdfreader.pages):
    content = page.extract_text()
    if content:
        raw_text += content

raw_text

"""### Text Splitting:
Since the extracted text can be lengthy, it's split into smaller chunks using CharacterTextSplitter.

OpenAI's, have token limits, so the text needs to be split into manageable chunks.

The chunk_size defines the maximum length of each chunk, while chunk_overlap allows some overlap between chunks to maintain context.
"""

# We need to split the text using Character Text Split such that it sshould not increse token size
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 800,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

len(texts)

"""### Generating Embeddings:
OpenAI Embeddings: The code downloads embeddings from OpenAI, which are used to convert the text chunks into numerical vectors. These embeddings are essential for later querying or searching through the text based on semantic meaning.
"""

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

!pip install tenacity
import time
from tenacity import (
    retry,
    stop_after_attempt,
    wait_random_exponential,
)  # for exponential backoff

"""**Embeddings:** Numerical representations of text that capture its semantic meaning. These are crucial for tasks like text similarity, clustering, and search."""

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings(allowed_special=['<|endofprompt|>'])

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def embed_with_retry(texts, embeddings):
    # Add a delay here if needed
    time.sleep(1)  # Example: wait for 1 second between calls
    document_search = FAISS.from_texts(texts, embeddings)
    return document_search

document_search = embed_with_retry(texts, embeddings)

"""### Vector Database (FAISS):

A database that stores vectors (embeddings) and allows for efficient similarity search. In this case, FAISS is used to store and query the embeddings of the text chunks.
"""

document_search = FAISS.from_texts(texts, embeddings)

document_search

query = "blessy previous employer"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "what are blessy's skills"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)

"""# Legal Document Summarization with Langchain
## Overview:
This script extracts, processes, and summarizes content from legal documents (e.g., lease contracts) using Langchain and OpenAI embeddings. It efficiently identifies and retrieves key information for faster document review.
"""

### For reading big PDF's

pdfreader = PdfReader('Lease_Agreement_178_Bluxome_.pdf')

# read text from pdf
raw_text = ''
for i, page in enumerate(pdfreader.pages):
    content = page.extract_text()
    if content:
        raw_text += content


# We need to split the text using Character Text Split such that it sshould not increse token size
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 800,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_text)



# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings()

# Download embeddings from OpenAI
embeddings = OpenAIEmbeddings(allowed_special=['<|endofprompt|>'])

@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))
def embed_with_retry(texts, embeddings):
    # Add a delay here if needed
    time.sleep(1)  # Example: wait for 1 second between calls
    document_search = FAISS.from_texts(texts, embeddings)
    return document_search

document_search = embed_with_retry(texts, embeddings)


document_search = FAISS.from_texts(texts, embeddings)

query = "Summarise the lease agreement in 3 lines"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "when does the bluxome lease end"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)

query = "who are there on the lease give names"
docs = document_search.similarity_search(query)
chain.run(input_documents=docs, question=query)